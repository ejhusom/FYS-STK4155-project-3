\section{Discussion}\label{sec:Discussion}
For setting 1 (mixed subjects) we found that all of our classifiers performed almost perfectly, with accuracy scores in the $99$-th percentile on the test data in almost all of the cases (table \ref{tab:accuracy}). The single decision tree performed almost equally as well as the ensemble and boosting methods, which at first thought is quite surprising, though it's configuration is of a high complexity, with a tree depth of 19. We achieve high accuracy scores with all of our classifiers, but the parameter configurations that were found to be best, were in most cases an edge case of the grid search. Due to the high computational cost of further reducing the error, we have opted to not expand the grid search. 

Although there is very low variance in the error of our models for the mixed subjects case, there is good reason to suspect that the models are overfitted to the movement patterns of these 15 subjects, due to the training data being very similar to the test data. This suspicion is very much supported by our results when training and testing the models on separate subjects, where we from table \ref{tab:accuracy} see that the prediction accuracy is considerably lower for all the models, with a simple decision tree being the worst with $7.2\%$ accuracy, and the gradient boosted trees the best with $19.5\%$. Since we have used data from the same subjects for training and validation, it is a challenging task to tune the hyper parameters of the models in this manner such that they generalize to the movement patterns of other subjects. A better approach to performing this task may have been to use data from separate subjects in training, validation and testing, as this likely would reduce the high variance for more biased models which generalize better. 

Figure \ref{fig:feature_importance} gives an indication of what features are deemed as most important by the various algorithms. The ranges of all axes are in general not contributing much to the model, while features like the standard deviation of the acceleration in $y$-direction and the velocity in $y$-direction are given high significance. These results might suggest that some features could be dropped from the dataset without worsening the results. However, because of the poor performance of the models in setting 2, it could be argued that adding additional features to the set is worth exploring; for example, some researches have had success with using high- and low-frequency filters on the accelerometer data\cite{Casale2011}. Even so, the main findings of this analysis suggests that overfitting of the training data is primary problem.

Comparing our results with those of Ravi et al.\cite{ravi} in table \ref{tab:accuracy}, we see that their decision tree, bagging and boosting classifiers achieve very similar accuracy scores as ours when mixing the subjects (setting 1), but there is a big difference in classification accuracy when separate subjects are used for training and testing (setting 2). It is not clear in their paper which hyper parameters they have used in their models, but we suspect that they are composed of trees with a considerably higher bias. They have also included correlation between the axes as predictors, but we chose to leave it out since our initial exploration of adding this feature seemed fruitless. Additionally, Ravi et al. used a different (though similar) dataset, so it is difficult to provide a direct comparison.